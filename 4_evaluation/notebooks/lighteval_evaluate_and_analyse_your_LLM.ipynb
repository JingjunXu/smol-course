{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZVw9f5QYWPL"
      },
      "source": [
        "# lighteval is your AI evaluation library\n",
        "\n",
        "This notebook explores how you can use lighteval to evaluate and compare LLMs.\n",
        "\n",
        "`lighteval` has been around a while and it's a great tool for getting eval score on major benchmarks. It's just been refactored to support being used like a library in Python, which makes it great for comparing models across benchmarks.\n",
        "\n",
        "So let's dig in to some eval scores.\n",
        "\n",
        "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
        "    <h2 style='margin: 0;color:blue'>Exercise: Evaluate Your Own Model</h2>\n",
        "    <p>Now that you've seen how to evaluate models on specific domains, try evaluating a model on a domain that interests you.</p>\n",
        "    <p><b>Difficulty Levels</b></p>\n",
        "    <p>üê¢ Use the existing medical domain tasks but evaluate a different model from the Hugging Face hub</p>\n",
        "    <p>üêï Create a new domain evaluation by selecting different MMLU tasks (e.g., computer science, mathematics, physics)</p>\n",
        "    <p>ü¶Å Create a custom evaluation task using LightEval's task framework and evaluate models on your specific domain</p>\n",
        "</div>\n",
        "\n",
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afW_CLJoPCnF"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq -U \"torch<2.5\" \"torchvision<2.5\" \"torchaudio<2.5\" --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip list | grep torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xMnn_cQ1EEi",
        "outputId": "68b6d3f3-47a7-43f5-8a45-d0842d17a8d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qqq git+https://github.com/huggingface/lighteval.git tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDKs5ShvXw8K"
      },
      "source": [
        "## Setup `lighteval` evaluation\n",
        "\n",
        "We need to setup the evaluation environment and pipeline. Much of this we will disable because we're keeping things in the notebook, but we could also use `push_to_hub` or `push_to_tensorboard`.\n",
        "\n",
        "### `push_to_hub`\n",
        "\n",
        "This is useful if we're evaluating a model and want to persist its evaluation with weights and configuration on the Hugging Face hub.\n",
        "\n",
        "### `push_to_tensorboard`\n",
        "\n",
        "This would be useful if we were building an evaluation tool or script, where we wanted to view results within tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "L2PZjYYrbv5i",
        "outputId": "ef6690ae-6930-4c91-deea-5384da94d9ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==2.0.0"
      ],
      "metadata": {
        "id": "IE5JblxJb2-d",
        "outputId": "c10fbf2a-d2bb-4347-dbe0-edf352331a7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==2.0.0\n",
            "  Using cached numpy-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Using cached numpy-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lighteval 0.8.1.dev0 requires numpy<2, but you have numpy 2.0.0 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "3cUebd-z6IWs",
        "outputId": "65f93f86-a6e6-4cde-8c98-e210580fa1c1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lighteval.logging.hierarchical_logger'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-2e9783eb86ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlighteval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_tracker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlighteval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhierarchical_logger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhlog_warn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtrack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlighteval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_model_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlighteval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParallelismManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPipelineParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lighteval.logging.hierarchical_logger'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import lighteval\n",
        "import os\n",
        "from datetime import timedelta\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
        "from lighteval.logging.hierarchical_logger import hlog_warn, htrack\n",
        "from lighteval.models.model_config import create_model_config\n",
        "from lighteval.pipeline import EnvConfig, ParallelismManager, Pipeline, PipelineParameters\n",
        "\n",
        "TOKEN = os.getenv(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muikmXNQXgFv"
      },
      "outputs": [],
      "source": [
        "env_config = EnvConfig(token=TOKEN, cache_dir=\"~/tmp\")\n",
        "\n",
        "evaluation_tracker = EvaluationTracker(\n",
        "    output_dir=\"~/tmp\",\n",
        "    save_details=False,\n",
        "    push_to_hub=False,\n",
        "    push_to_tensorboard=False,\n",
        "    public=False,\n",
        "    hub_results_org=False,\n",
        ")\n",
        "\n",
        "pipeline_params = PipelineParameters(\n",
        "    launcher_type=ParallelismManager.ACCELERATE,\n",
        "    env_config=env_config,\n",
        "    job_id=1,\n",
        "    override_batch_size=1,\n",
        "    num_fewshot_seeds=0,\n",
        "    max_samples=10,\n",
        "    use_chat_template=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsNjwzCtltkA"
      },
      "source": [
        "# Compares models with `lighteval`\n",
        "\n",
        "We are going to compare two small LLMs on a domain. We will use `Qwen2.5-0.5B` and `SmolLM2-360M-Instruct` and we will evaluate them on a medical domain.\n",
        "\n",
        "We can create a domain evaluation from a subset of MMLU evaluations, by defining the evaluation tasks. In lighteval, tasks are described as strings.\n",
        "\n",
        "`{suite}|{task}:{subtask}|{num_few_shot}|{0 or 1 to reduce num_few_shot if prompt is too long}`\n",
        "\n",
        "Therefore, we will pass our list of medicine related tasks like this:\n",
        "\n",
        "```\n",
        "\"leaderboard|mmlu:anatomy|5|0,leaderboard|mmlu:professional_medicine|5|0,leaderboard|mmlu:high_school_biology|5|0,leaderboard|mmlu:high_school_chemistry|5|0\"\n",
        "```\n",
        "\n",
        "Which can be translated to :\n",
        "\n",
        "| Suite | Task | Num Fewshot Example | Limit Fewshots |\n",
        "|---|---|---|---|\n",
        "| leaderboard | mmlu:anatomy | 5 | False |\n",
        "| leaderboard | mmlu:professional_medicine | 5 | False |\n",
        "| leaderboard | mmlu:high_school_biology | 5 | False |\n",
        "| leaderboard | mmlu:high_school_chemistry | 5 | False |\n",
        "\n",
        "For a full list of lighteval supported tasks. Checkout this page in [the documentation](https://github.com/huggingface/lighteval/wiki/Available-Tasks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTqsizv9mVbO"
      },
      "outputs": [],
      "source": [
        "domain_tasks = \"leaderboard|mmlu:anatomy|5|0,leaderboard|mmlu:professional_medicine|5|0,leaderboard|mmlu:high_school_biology|5|0,leaderboard|mmlu:high_school_chemistry|5|0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwcJklSFX4H6"
      },
      "source": [
        "# Evaluate Qwen2.5 0.5B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXJuaXVxUNBO"
      },
      "outputs": [],
      "source": [
        "qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    tasks=domain_tasks,\n",
        "    pipeline_parameters=pipeline_params,\n",
        "    evaluation_tracker=evaluation_tracker,\n",
        "    model=qwen_model\n",
        ")\n",
        "\n",
        "pipeline.evaluate()\n",
        "\n",
        "qwen_results = pipeline.get_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIwCaCxJX_hA"
      },
      "source": [
        "# Evaluate SmolLM 360M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxg0RtlNVT4y"
      },
      "outputs": [],
      "source": [
        "smol_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    tasks=domain_tasks,\n",
        "    pipeline_parameters=pipeline_params,\n",
        "    evaluation_tracker=evaluation_tracker,\n",
        "    model=smol_model\n",
        ")\n",
        "\n",
        "pipeline.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdjyzfKHVt52"
      },
      "outputs": [],
      "source": [
        "smol_results = pipeline.get_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eugvMFfgV1VD"
      },
      "outputs": [],
      "source": [
        "pipeline.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HD8aFwSYGHu"
      },
      "source": [
        "# Visualize Results\n",
        "\n",
        "Now that we have results from the two models we can visualize them side-by-side. We'll keep visualisation simple here, but with this data structure you could represent scores in many ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sReqrgQUO9r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_records(smol_results[\"results\"]).T[\"acc\"].rename(\"SmolLM2-360M-Instruct\")\n",
        "_df = pd.DataFrame.from_records(qwen_results[\"results\"]).T[\"acc\"].rename(\"Qwen2-0.5B-DPO\")\n",
        "df = pd.concat([df, _df], axis=1)\n",
        "df.plot(kind=\"barh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJEbQeYDplKX"
      },
      "source": [
        "# üíê That's it!\n",
        "\n",
        "You have a handy notebook to view model evals. You could use this to:\n",
        "\n",
        "- select the right model for your inference use case\n",
        "- evaluate checkpoints during training\n",
        "- share model scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWdS38syaipm"
      },
      "source": [
        "üèÉNext Steps\n",
        "\n",
        "- If you want to go deeper into your evaluation results check out this [notebook](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/examples/comparing_task_formulations.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}